\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{wrapfig}

\newcommand\abs[1]{\left|#1\right|}
\newcommand{\R}{\mathbb{R}}
\newcommand\ttt\texttt
\renewcommand\arraystretch{1.5}

\setlength{\parindent}{0pt}

\title{Sparse Support Vector Machines with PySCIPOpt}

\begin{document}

\maketitle

\section*{Introduction}

Given a set $X$ of $d$-dimensional data consisting of two types of points, we want to find a function that classifies each point to be in one of the two sets based on its location.
We want work with the hypothesis of a \emph{linear classification model}, i.e.\ we look for a hyperplane that separates these two sets\footnote{graphic taken from: https://en.wikipedia.org/wiki/File:Svm\_max\_sep\_hyperplane\_with\_margin.png.}.

\begin{wrapfigure}{r}{.5\linewidth}
  \includegraphics[width=.5\textwidth, keepaspectratio]{img/svm_max_sep_hyperplane.png}
\end{wrapfigure}

A hyperplane $h$ is given by a normal vector $\omega$ and a translation $b$ and the classification $h_{\omega, b}$ is defined as follows:
$$ h_{\omega, b}(x) = \text{sgn} (\omega^{T}x + b) \in \{-1, 1\}$$

We require that this evaluation coincides with the given classification of the points\footnote{
This may not always be possible.
In that unlucky case we require the condition for as many datapoints $x$ as possible and penalize misclassifications.}.


Additionally we require the parameter $\omega$ and $b$ to be such that no points lie in the \emph{margin} which is defined as the following set of points $x$\footnote{
It is easy to see that the width of the margin decreases when the length of $\omega$ increases.}:

$$ \left\{ x : \abs{h_{\omega,b} (x)} < 1 \right\} $$

We want to reduce the dimension of the original space and consider only a subset of features\footnote{
  This classification to be successful meaning that not all features are relevant.}.
In our model this implies that a certain fraction of weights is required to be zero resulting in a \emph{sparse classifier}.


A sparse classifier with sparsity $\rho$ is a linear classifier where a fraction of the $\omega$ entries is equal to 0:
$$ \rho(\omega) = \frac{\abs{\{ i \colon \omega_{i} = 0 \}}}{d} $$

Advantages of a sparse classifier are \emph{a smaller cost} of the classification and the fact that it results in a \emph{simpler model}\footnote{\emph{Occam's razor}: from a set of solutions to a problem select the one that makes the fewest assumptions.}


\clearpage
\section*{Optimization model - linear SVM}

Let the set of datapoints consist of $n$ $d$-dimensional features $X \in \R^{n,d}$, labeled by $y \in \{-1,+1\}^{n}$ and let $C > 0$ be a regularization parameter.
As a loss function, we consider the \emph{Hinge loss}:
$$l^{i}(t) :=  \max \{0, 1 - y^i t\} \text{\quad for } i \in \{1,\dots, n\}$$

We will use this function to penalize wrongly classified datapoints.
As we want to minimize the penalty, and maximize the margin (equivalently minimize the length of $\omega$), our model can now be written as the following optimization problem:

$$
\min\limits_{\omega, b}
\frac{C}{n} \sum\limits_{i = 1}^{n} l^i(\omega^{T}X^{i} + b)
+\frac{1}{2} \lVert \omega \rVert_{2}^{2}
$$

Substituting the Hinge loss for a variable

$$ \begin{aligned}
\xi^i &\leq l^i( \omega^T X^i + b ) \\
&= \max \{0, 1 - y^i (\omega^TX^i + b) \},
\end{aligned} $$

the above problem is equivalent to:

\begin{equation*}
\begin{array}{rll}
  \min\limits_{\omega, b}
  & \frac{C}{n} \sum\limits_{i = 1}^n \xi^i + \frac{1}{2} \lVert \omega \rVert_2^2
\\
  \text{such that}
  & 1 - y^i (\omega^T X^i +b) \leq \xi^i,
  & i \in \{1,\dots,n\}
\\
  & 0 \leq \xi^i,
  & i \in \{1,\dots, n\}
\end{array}
\end{equation*}

\section*{Model - Sparse linear SVM}

To implement a sparse classifier with sparsity $\rho$, we add additional constraints and variables to the model\footnote{
  Another possibility would be to prefer sparse solutions using an $L1$-norm in the objective function.}.

\begin{equation*}
\begin{array}{rll}
  & \sum_{j \in d} v_{j} \leq \rho \cdot d
  &
\\
  & -B \cdot v_{j} \leq \omega_{j} \leq B \cdot v_{j},
  & j \in \{ 1, \dots, d \}
\\
  & v_{j} \in \{0,1\},
  & j \in \{1,\dots, d\}
\end{array}
\end{equation*}

For $i \in \{ 1, \dots, d \}$ we assume the weights $\omega_j$ to be bounded by $-B$ and $B$ for a bound $B > 0$.
Only a fraction of these new binary indicator variables $v_j$ are allowed to be nonzero.
Then all the $v_j$ that are zero will force their corrensponding $\omega_j$ to be zero.

\section*{Balancing data}

Depending on the number of positive and negative samples in the data we might want to weight the penalties differently, ensuring that points from one of the sets have a higher probability to be classified correctly\footnote{
  An application would be medical tests, where the a false negative should be highly unlikely, whereas a false positive is not disastrous.}.
This correction $c_i$ is applied in the objective function:

$$
\frac{C}{n} \sum\limits_{i = 1}^n c_i \xi^i + \frac{1}{2} \lVert \omega \rVert_2^2
\text{,\quad where } c_i =
\begin{cases}
  \alpha & \text{ if } y_i = 1 \\
  \beta & \text{ if } y_i = -1
\end{cases}
$$

\section*{Material}

This exercise with solution can be found on
\begin{center}
  \ttt{git clone https://github.com/fschloesser/scip-workshop}
\end{center}

The sub directory \ttt{scip-workshop/support-vector-machine} contains a subdirectory \ttt{data} and a python file \ttt{svm.py}.

The dataset for training is the classification of benign ($y=-1$) or malignant ($y=1$) breast cancer based on 30 features and contains 569 training samples.
It is taken from:

\begin{center}
  \ttt{\footnotesize http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/}
\end{center}


\end{document}

