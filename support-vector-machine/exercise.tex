\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{listings}
\usepackage{paralist}

\newcommand\abs[1]{\left|#1\right|}
\newcommand{\R}{\mathbb{R}}
\newcommand\ttt\texttt
\renewcommand\arraystretch{1.5}

\setlength{\parindent}{0pt}

\begin{document}

\section*{Sparse Support Vector Machines with PySCIPOpt}

\subsection*{Support Vector Machines}

Given a set $X$ of $d$-dimensional data consisting of two types of points, we want to find a function that classifies each point to be in one of the two sets based on its location.
We want work with the hypothesis of a \emph{linear classification model}, i.e.\ we look for a hyperplane that seperates these two sets.
A hyperplane $h$ is given by a normal vector $\omega$ and a translation $b$ and shall fulfill the following equality for as many datapoints $x$ as possible:
$$ h_{\omega, b}(x) = \text{sgn} (\omega^{T}x + b) $$

Using a \emph{sparse classifier} we want to consider a subset of features for the classification, meaning that only a certain fraction of weights can be nonzero.

A sparse classifier with sparsity $\rho$ consists of weights $\omega$ and an offset $b$ where a fraction of the $\omega$ entries is equal to 0:
$$ \rho(\omega) = \frac{\abs{\{ i \colon \omega_{i} = 0 \}}}{d} $$

Advantages of a sparse classifier are \emph{a smaller cost} of the classification and the fact that it results in a \emph{simpler model}\footnote{\emph{Occam's razor}: from a set of solutions to a problem select the one that makes the fewest assumptions.}.

\subsection*{Optimization model - linear SVM}

Let the set of datapoints consist of $n$ $d$-dimensional features $X \in \R^{n,d}$, labeled by $y \in \{-1,+1\}^{n}$ and let $C > 0$ be a regularization parameter.
As a loss function, we consider the \emph{Hinge loss}:
$$l^{i}(t) :=  \max \{0, 1 - y^i t\} \text{ for } i \in \{1,\dots, n\}$$

Our support vector machine can then be written as the following minimization problem:

$$
\min\limits_{\omega, b}
\frac{C}{n} \sum\limits_{i = 1}^{n} l^i(\omega^{T}X^{i} + b)
+\frac{1}{2} \lVert \omega \rVert_{2}^{2}
$$

Substituting the Hinge loss for a variable

$$ \begin{aligned}
\xi^i :&= l^i( \omega^T X^i + b ) \\
&= \max \{0, 1 - y^i (\omega^TX^i + b) \},
\end{aligned} $$

the problem is equivalent to:

\begin{equation*}
\begin{array}{rll}
  \min\limits_{\omega, b}
  & \frac{C}{n} \sum\limits_{i = 1}^n \xi^i + \frac{1}{2} \lVert \omega \rVert_2^2
\\
  \text{such that}
  & 1 - y^i (\omega^T X^i +b) \leq \xi^i,
  & i \in \{1,\dots,n\}
\\
  & 0 \leq \xi^i,
  & i \in \{1,\dots, n\}
\end{array}
\end{equation*}

\clearpage
\subsection*{Model - Sparse linear SVM}

To implement a sparse classifier with sparsity $\rho$, we add additional constraints and variables to the model\footnote{
  Another possibility would be to prefer sparse solutions using an $L1$-norm in the objective function.}.

\begin{equation*}
\begin{array}{rll}
  \min\limits_{\omega, b, v}
  & \frac{C}{n} \sum\limits_{i = 1}^{n} \xi^{i} + \frac{1}{2} \lVert \omega \rVert_{2}^{2}
  &
\\
  \text{such that}
  & 1 - y^i (\omega^T X^i +b) \leq \xi^i,
  & i \in \{1,\dots,n\}
\\
  & 0 \leq \xi^i,
  & i \in \{1, \dots, n\}
\\
  & \sum_{j \in d} v_{j} \leq \rho \cdot d
  &
\\
  & -B \cdot v_{j} \leq \omega_{j} \leq B \cdot v_{j},
  & j \in \{ 1, \dots, d \}
\\
  & v_{j} \in \{0,1\},
  & j \in \{1,\dots, d\}
\end{array}
\end{equation*}

For $i \in \{ 1, \dots, d \}$ we assume the weights $\omega_j$ to be bounded by $-B$ and $B$ for a bound $B > 0$.
Only a fraction of these new binary indicator variables $v_j$ are allowed to be nonzero.
Then all the $v_j$ that are zero will force their corrensponding $\omega_j$ to be zero.


\section*{Quickstart to PySCIPOpt}

PySCIPOpt is the Python interface for SCIP.
It allows for \emph{fast model prototyping} with \emph{flexible expressions}.
Additionally PySCIPOpt supports \emph{user-plugins} similar to C.

\lstset{language=python,%
basicstyle=\sffamily\footnotesize,%
numberstyle=\sffamily\tiny\color{siennabrown},stepnumber=1}
\begin{lstlisting}[frame=tb]{}
  from pyscipopt import Model

  # initialize model
  m = Model("svm")

  # add variables
  x = m.addVar(vtype='B', name='x')
  y = m.addVar(vtype='C', name='y')

  # add a constraint
  m.addCons(3 * x + 2 * y >= 4)

  m.optimize()
\end{lstlisting}

\subsection*{Adding variables}

Different variable types \ttt{vtype} are supported by \ttt{addVars}: \ttt{B} (binary), \ttt{I} (integer), \ttt{C} (continuous).
You can specify lower and upper bounds: \ttt{lb} (default: 0), \ttt{ub} (default: \ttt{None} $\sim \infty$), as well as objective coefficients: \ttt{obj} (default: $0.0$).

\lstset{language=python,%
basicstyle=\sffamily\footnotesize,%
numberstyle=\sffamily\tiny\color{siennabrown},stepnumber=1}

\begin{lstlisting}[frame=tb]{}
  m = Model("svm")

  # add variables
  x = m.addVar(vtype='C', name='x', lb=-10, ub=10, obj = 1.0)
\end{lstlisting}

\clearpage
\subsection*{Nonlinear objective functions}
SCIP can only handle linear objective functions.
Therefore a nonlinear objective function must be transformed into a constraint using an auxiliary variable:
$$ \text{min } f(x) \; \Leftrightarrow \; \text{min } t \text{ such that } f(x) \leq t $$

\subsection*{Adding constraints}

\ttt{addCons} understands expression objects and requires an operator \ttt{<=}, \ttt{>=}, \ttt{==}. Variables and constants can appear on both sides of the operator.
For complex expressions use the \ttt{quicksum} summation and python list comprehension \ttt{[a[i] for i in range(5)]}, multiplying variables yields nonlinear constraints,

\begin{lstlisting}[frame=tb]{}
  from pyscipopt import Model, quicksum
  m = Model("svm")
  vars = []
  for i in range(10):
      vars.append(m.addVar(vtype='B', name=str(i)))
  m.addCons(vars[0] ** 2 == 1)
  m.addCons(quicksum(i * vars[i] for i in range(10) <= 15),
            name="mycons")

  m.optimize()
\end{lstlisting}


\section*{Material}

This exercise with solution can be found on
\begin{center}
  \ttt{git clone https://github.com/fschloesser/scip-workshop}
\end{center}

The sub directory \ttt{scip-workshop/support-vector-machine} contains a subdirectory \ttt{data} and a python file \ttt{svm.py}.

The dataset for training is the classification of benign ($y=-1$) or malignant ($y=1$) breast cancer based on 30 features and contains 569 training samples.
It is taken from:

\begin{center}
  \ttt{\footnotesize http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/}
\end{center}


\end{document}

